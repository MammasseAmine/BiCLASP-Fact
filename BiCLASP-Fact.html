<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Code Sections</title>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h2, h3 {
            margin-top: 1.5em;
            border-bottom: 1px solid #eee;
            padding-bottom: 0.3em;
        }
        pre {
            background-color: #f8f8f8;
            border: 1px solid #ccc; /* Mimics frame=single */
            padding: 12pt;          /* Mimics framesep=12pt */
            width: 98%;             /* Mimics linewidth=0.98\textwidth */
            margin: 1em auto;       /* Centers the block */
            overflow-x: auto;       /* Handle long lines */
            box-sizing: border-box;
        }
        /* Style specific to code block content */
        pre code.language-python {
            font-family: monospace;
            white-space: pre; /* Crucial: Preserves whitespace and line breaks */
            display: block; /* Ensure code block takes up pre area */
            color: #333; /* Optional: set base code color */
        }
         /* Style for inline code */
        code:not(pre code) { /* Target only inline code */
            background-color: #eee;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: monospace;
        }
        p {
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }
    </style>
    <!-- Optional: Add a syntax highlighter library like Prism.js or highlight.js -->
    <!-- Example using Prism.js (download or link from CDN)
    <link href="path/to/prism.css" rel="stylesheet" />
    <script src="path/to/prism.js"></script>
    -->
</head>
<body>

    <h2>Core Libraries Imports</h2>
    <pre><code class="language-python"># --- Core Libraries ---
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import random_split
from torch.optim.lr_scheduler import CosineAnnealingLR
import math
import os
import re
from collections import Counter
from datetime import datetime
import json
import requests # For fetching external data
import time # Added for Continuous Learning Pipeline delays
import torch.nn.functional as F # Added for loss/softmax functions
</code></pre>

    <h2>Hugging Face Transformers Imports</h2>
    <pre><code class="language-python">from transformers import AutoTokenizer, AutoModel
</code></pre>

    <h2>Scikit-learn Imports</h2>
    <pre><code class="language-python">from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, f1_score,precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay)
from sklearn.feature_selection import mutual_info_classif
</code></pre>

    <h2>Explainability Imports</h2>
    <pre><code class="language-python">import shap
from lime.lime_text import LimeTextExplainer
import matplotlib.pyplot as plt # Needed for SHAP plots
</code></pre>

    <h2>PyTorch Quantization Imports</h2>
    <pre><code class="language-python">import torch.ao.quantization as quantization
</code></pre>

    <h2>Mixed Precision Training Imports</h2>
    <pre><code class="language-python">from torch.cuda.amp import autocast, GradScaler
</code></pre>

    <h2>Utilities Imports</h2>
    <pre><code class="language-python">from tqdm import tqdm # Progress bars
import torch.onnx # For ONNX export
import onnx # For ONNX validation
</code></pre>

    <h2>SEED for Reproducibility</h2>
    <pre><code class="language-python">SEED = 42

def set_seed(seed_value):
    torch.manual_seed(seed_value)
    np.random.seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)
        # Ensure deterministic behavior in PyTorch
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(SEED)
</code></pre>

    <h2>Device Setup</h2>
    <pre><code class="language-python">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
</code></pre>

    <h2>Data Loading</h2>
    <pre><code class="language-python"># --- Load Arabic Dataset ---
arabic_csv_path = "ArabicGlobalFakeNewsData.csv"
arabic_df = pd.read_csv(arabic_csv_path)
arabic_df = arabic_df.rename(columns={"Text": "statement", "label": "Label"})

# --- Load English Dataset ---
english_csv_path = "EnglishGlobalFakeNewsData.csv" 
english_df = pd.read_csv(english_csv_path)
</code></pre>

    <h2>Split Datasets</h2>
    <pre><code class="language-python">english_train_df, english_test_df = train_test_split(english_df, test_size=0.2, random_state=SEED)

arabic_train_df, arabic_test_df = train_test_split(arabic_df, test_size=0.2, random_state=SEED)
</code></pre>

    <h2>Combine Datasets</h2>
    <pre><code class="language-python">combined_train_df = pd.concat([english_train_df, arabic_train_df], ignore_index=True)
combined_test_df = pd.concat([english_test_df, arabic_test_df], ignore_index=True)

# Define column names used later
text_column = 'statement'
label_column = 'Label'
</code></pre>

    <h2>Tokenizer and Base Embedding Models</h2>
    <pre><code class="language-python">byt5_tokenizer = AutoTokenizer.from_pretrained("google/byt5-xxl")

gte_model = AutoModel.from_pretrained(
    "Alibaba-NLP/gte-multilingual-mlm-base",
    trust_remote_code=True
)

gte_model.to(device)
# Set GTE model to evaluation mode as it's only used for embedding generation
gte_model.eval()
</code></pre>

    <h2>Custom Dataset and DataLoader</h2>
    <pre><code class="language-python">class FakeNewsDataset(Dataset):
    """
    PyTorch Dataset for fake news classification.
    Generates embeddings using a provided base model (GTE) during item retrieval.
    Moves data back to CPU before returning for DataLoader compatibility.
    """
    def __init__(self, texts, labels, tokenizer,
    base_embedding_model, max_length=512):
        self.texts = texts.reset_index(drop=True)
        self.labels = labels.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.base_embedding_model = base_embedding_model
        self.max_length = max_length
        # Get device from model
        self.device = next(
        base_embedding_model.parameters()).device

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]

        # Tokenize text - no padding here, handled by collate_fn
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            padding=False, # Dynamic padding in collate_fn
            truncation=True,
            max_length=self.max_length
        )
        # Move tokenized inputs to the same device as the base model
        input_ids = inputs['input_ids'].to(self.device)
        attention_mask = inputs['attention_mask'].to(self.device)

        with torch.no_grad():
            outputs = self.base_embedding_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            embeddings = outputs.last_hidden_state.squeeze(0)

        # Move generated embeddings and attention mask back to CPU for DataLoader
        embeddings_cpu = embeddings.cpu()
        attention_mask_cpu = attention_mask.squeeze(0).cpu()

        return {
            'embeddings': embeddings_cpu,
            'attention_mask': attention_mask_cpu,
            'label': torch.tensor(label, dtype=torch.long)}
</code></pre>

    <h2>Custom Collate Function</h2>
    <p>Defining a function to dynamically pad sequences within each batch to the maximum length in that batch.</p>
    <pre><code class="language-python">def custom_collate_fn(batch):
    """
    Collates batch data, padding embeddings and attention masks dynamically.
    Input: list of dictionaries from FakeNewsDataset.__getitem__
    Output: dictionary of batched tensors.
    """
    embeddings_list = [item['embeddings'] for item in batch]
    attention_masks_list = [item['attention_mask'] for item in batch]
    labels_list = [item['label'] for item in batch]

    # Find max sequence length in this batch
    max_seq_len = max(emb.size(0) for emb in embeddings_list)
    batch_size = len(batch)
    hidden_dim = embeddings_list[0].size(1)

    # Create zero tensors for padding
    padded_embeddings = torch.zeros(batch_size, max_seq_len, hidden_dim, dtype=embeddings_list[0].dtype)
    padded_attention_masks = torch.zeros(batch_size, max_seq_len, dtype=torch.long)

    # Fill padded tensors
    for i, (emb, mask) in enumerate(zip(embeddings_list, attention_masks_list)):
        seq_len = emb.size(0)
        padded_embeddings[i, :seq_len, :] = emb
        padded_attention_masks[i, :seq_len] = mask

    # Stack labels
    labels = torch.stack(labels_list)

    return {
        'embeddings': padded_embeddings,
        'attention_mask': padded_attention_masks,
        'labels': labels
    }
</code></pre>

    <h2>Function to Create DataLoaders</h2>
    <p>A helper function to instantiate the <code>FakeNewsDataset</code>, split it into training and validation sets, and create PyTorch <code>DataLoader</code> objects.</p>
    <pre><code class="language-python">def create_dataloaders(texts, labels, tokenizer, base_embedding_model, batch_size=64, train_split=0.8):
    """Creates train and validation dataloaders from text and label data."""
    dataset = FakeNewsDataset(texts, labels, tokenizer, base_embedding_model)

    # Split dataset into training and validation
    total_size = len(dataset)
    train_len = int(train_split * total_size)
    val_len = total_size - train_len
    generator = torch.Generator().manual_seed(SEED)
    train_dataset, val_dataset = random_split(dataset, [train_len, val_len], generator=generator)

    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Validation dataset size: {len(val_dataset)}")

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=custom_collate_fn,
        pin_memory=True,
        num_workers=2
    )
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False, # No need to shuffle validation data
        collate_fn=custom_collate_fn,
        pin_memory=True,
        num_workers=2
    )
    return train_dataloader, val_dataloader
</code></pre>

    <h2>Create Training and Validation DataLoaders</h2>
    <pre><code class="language-python">BATCH_SIZE = 64
train_dataloader, val_dataloader = create_dataloaders(
    combined_train_df[text_column],
    combined_train_df[label_column],
    byt5_tokenizer,
    gte_model,
    batch_size=BATCH_SIZE
)
</code></pre>

    <h2>Create Combined Test DataLoader</h2>
    <pre><code class="language-python">combined_test_dataset = FakeNewsDataset(
    combined_test_df[text_column],
    combined_test_df[label_column],
    byt5_tokenizer,
    gte_model
)
combined_test_dataloader = DataLoader(
    combined_test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=custom_collate_fn,
    pin_memory=True,
    num_workers=2
)
</code></pre>

    <h2>Model Architecture Definition</h2>

    <h3>Fourier Positional Encoding</h3>
    <pre><code class="language-python">class FourierPositionalEncoding(nn.Module):
    """Adds Fourier features for positional encoding."""
    def __init__(self, d_model, temperature=10000, max_len=1024):
        super().__init__()
        self.d_model = d_model
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(temperature) / d_model))

        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        # x shape: [batch_size, seq_len, embedding_dim]
        seq_len = x.size(1)

        # Select positional embeddings up to
        # the sequence length
        if seq_len &gt; self.pe.size(1):
            raise ValueError(f"Sequence length {seq_len} \ exceeds max_len {self.pe.size(1)} in positional encoding.")

        # Add positional encoding (sliced to match seq_len)
        return x + self.pe[:, :seq_len, :]
</code></pre>

    <h3>Layer Normalization</h3>
    <pre><code class="language-python">class LayerNormalization(nn.Module):
    """ Standard Layer Normalization implementation. """
    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):
        super().__init__()
        if isinstance(normalized_shape, int):
            normalized_shape = (normalized_shape,)
        self.normalized_shape = tuple(normalized_shape)
        self.eps = eps
        self.elementwise_affine = elementwise_affine
        if self.elementwise_affine:
            self.weight = nn.Parameter(
            torch.ones(self.normalized_shape))
            self.bias = nn.Parameter(
            torch.zeros(self.normalized_shape))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)

    def forward(self, x):
        # Normalize over the last dimensions specified by normalized_shape
        dims = tuple(-(i + 1) for i in range(len(self.normalized_shape)))
        mean = x.mean(dim=dims, keepdim=True)
        var = x.var(dim=dims, unbiased=False, keepdim=True)
        x_normalized = (x - mean) / torch.sqrt(var + self.eps)
        if self.elementwise_affine:
            x_normalized = x_normalized * self.weight + self.bias
        return x_normalized
</code></pre>

    <h3>Multi-Scale Self-Attention</h3>
    <pre><code class="language-python">class MultiScaleAttention(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.1):
        super().__init__()
        assert d_model % nhead == 0 # "d_model must be divisible by nhead" 
        self.d_model = d_model
        self.nhead = nhead
        self.d_head = d_model // nhead
        self.scale = math.sqrt(self.d_head)
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x, key_padding_mask=None):
        # x shape: [B, T, C] (Batch, Sequence Length, Embedding Dim)
        # key_padding_mask: [B, T], True indicates padding
        B, T, C = x.size()

        # 1. Linear projections and split into heads
        q = self.query(x).view(B, T, self.nhead, self.d_head).transpose(1, 2) # [B, nhead, T, d_head]
        k = self.key(x).view(B, T, self.nhead, self.d_head).transpose(1, 2) # [B, nhead, T, d_head]
        v = self.value(x).view(B, T, self.nhead, self.d_head).transpose(1, 2) # [B, nhead, T, d_head]

        # 2. Scaled Dot-Product Attention
        # (B, nhead, T, d_head) @ (B, nhead, d_head, T) -&gt; (B, nhead, T, T)
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale
        
        # 3. Apply key padding mask (mask padded tokens)
        # key_padding_mask: [B, T] -&gt; [B, 1, 1, T] for broadcasting
        if key_padding_mask is not None:
            attn_scores = attn_scores.masked_fill( key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))

        # 4. Softmax and Dropout
        attn_weights = F.softmax(attn_scores, dim=-1) # Used F here
        attn_weights = self.dropout(attn_weights)

        # 5. Apply attention to values
        # (B, nhead, T, T) @ (B, nhead, T, d_head)-&gt; (B, nhead, T, d_head)
        attn_output = torch.matmul(attn_weights, v)

        # 6. Concatenate heads and final projection
        # Transpose and contiguous first:
        # (B, T, nhead, d_head) then reshape
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)
        output = self.out_proj(attn_output)
        return output
</code></pre>

    <h3>Feed Forward Network</h3>
    <pre><code class="language-python">class FeedForwardNetwork(nn.Module):
    """ Standard Feed Forward Network for Transformer blocks. """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x
</code></pre>

    <h2>Encoder Layer</h2>
    <p>Defining a single Transformer Encoder layer, combining Multi-Head Attention, Feed Forward Network, Layer Normalization, and residual connections.</p>
    <pre><code class="language-python">class EncoderLayer(nn.Module):
    """ Single Transformer Encoder Layer. """
    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):
        super().__init__()
        self.self_attn = MultiScaleAttention(d_model, nhead, dropout=dropout)
        self.ffn = FeedForwardNetwork(d_model, dim_feedforward, dropout=dropout)
        self.norm1 = LayerNormalization(d_model)
        self.norm2 = LayerNormalization(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_key_padding_mask=None):
        # src_key_padding_mask: [B, T], Self Attention block
        src2 = self.self_attn(src, key_padding_mask= src_key_padding_mask)
        src = src + self.dropout(src2) # Residual connection
        src = self.norm1(src)          # Layer Norm

        # Feed Forward block
        src2 = self.ffn(src)
        src = src + self.dropout(src2) # Residual connection
        src = self.norm2(src)          # Layer Norm
        return src
</code></pre>

    <h2>Full Encoder</h2>
    <p>Stacking multiple <em>EncoderLayer</em> modules to form the complete Transformer Encoder.</p>
    <pre><code class="language-python">class Encoder(nn.Module):
    """ Stacks multiple Encoder Layers. """
    def __init__(self, d_model, nhead, dim_feedforward,
    num_layers, dropout=0.1):
        super().__init__()
        self.layers = nn.ModuleList([ EncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])

    def forward(self, src, src_key_padding_mask=None):
        output = src
        for layer in self.layers:
            output = layer(output, src_key_padding_mask=src_key_padding_mask)
        return output
</code></pre>

    <h2>Classifier Head</h2>
    <pre><code class="language-python">class ClassifierHead(nn.Module):
    """ Simple MLP classifier head. """
    def __init__(self, input_dim, num_classes, dropout_rate=0.3):
        super().__init__()
        hidden_dim = input_dim // 2
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.dropout(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x) # Apply dropout again before final layer
        x = self.fc2(x)
        return x
</code></pre>

    <h2>Combined Model</h2>
    <p>Assembling the complete classification model, including positional encoding, the encoder stack, classifier head, and quantization stubs.</p>
    <pre><code class="language-python">class CombinedModel(nn.Module):
    def __init__(self, embedding_dim=768, hidden_dim_ffn_multiplier=4, nhead=8, num_classes=2, num_encoder_layers=6, dropout=0.1):
        super().__init__()
        self.embedding_dim = embedding_dim
        dim_feedforward = embedding_dim * hidden_dim_ffn_multiplier

        # Quantization Stubs (for QAT)
        self.quant = torch.ao.quantization.QuantStub()
        self.dequant = torch.ao.quantization.DeQuantStub()

        # Model Components
        self.pos_encoder = FourierPositionalEncoding(embedding_dim, max_len=1024) # Increased max_len
        self.encoder = Encoder(embedding_dim, nhead, dim_feedforward, num_encoder_layers, dropout)
        self.classifier = ClassifierHead(embedding_dim, num_classes, dropout_rate=dropout)


    def forward(self, embeddings, attention_mask):
        # Apply Quantization Stub
        x = self.quant(embeddings)
        x = self.pos_encoder(x)
        # Create key padding mask (True where attention_mask is 0)
        src_key_padding_mask = (attention_mask == 0)
        encoder_output = self.encoder(x,
        src_key_padding_mask=src_key_padding_mask)

        # Masked Average Pooling
        # Expand mask to match encoder_output shape: [B, T, 1]
        mask_expanded = attention_mask.unsqueeze(-1).float()
        # Apply mask (zero out padded tokens)
        masked_encoder_output = encoder_output * mask_expanded
        # Sum along sequence length dimension
        sum_embeddings = masked_encoder_output.sum(dim=1)
        # Count non-padded tokens per sequence: [B, 1]
        lengths = attention_mask.sum(dim=1).unsqueeze(-1).float()
        # Avoid division by zero for empty sequences
        lengths = torch.clamp(lengths, min=1e-6)
        # Compute average
        pooled_output = sum_embeddings / lengths

        logits = self.classifier(pooled_output)
        # Apply Dequantization Stub
        logits = self.dequant(logits)

        return logits
</code></pre>

    <h2>Instantiate Model</h2>
    <pre><code class="language-python">model = CombinedModel(
    embedding_dim=gte_model.config.hidden_size,
    num_encoder_layers=6,
    nhead=8,
    hidden_dim_ffn_multiplier=4,
    num_classes=2
)
model.to(device)
</code></pre>

    <h2>Training Setup</h2>

    <h3>Optimizer and Scheduler Setup</h3>
    <pre><code class="language-python">def setup_optimizer_and_scheduler(model, learning_rate=2e-5, weight_decay=0.01, t_max=200):
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    # T_max is the number of iterations until the first restart (often set to total epochs or steps)
    scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=learning_rate/10)
    return optimizer, scheduler
</code></pre>

    <h3>Loss Function</h3>
    <pre><code class="language-python">criterion = nn.CrossEntropyLoss()
</code></pre>

    <h3>Instantiate Optimizer and Scheduler</h3>
    <pre><code class="language-python">LEARNING_RATE = 2e-5
NUM_EPOCHS_FOR_SCHEDULER = 10 # Example T_max (adjust based on training plan)
optimizer, scheduler = setup_optimizer_and_scheduler(
    model,
    learning_rate=LEARNING_RATE,
    t_max=NUM_EPOCHS_FOR_SCHEDULER
)
</code></pre>

    <h3>Initialize Mixed Precision Scaler</h3>
    <p>Setting up the GradScaler for automatic mixed-precision training to potentially speed up training and reduce memory usage on compatible GPUs.</p>
    <pre><code class="language-python">scaler = GradScaler(enabled=torch.cuda.is_available()
and device.type == 'cuda')
</code></pre>

    <h3>Training Function (One Epoch)</h3>
    <p>Defines the training logic for a single epoch, incorporating mixed precision.</p>
    <pre><code class="language-python">def train_one_epoch(model, dataloader, optimizer, criterion, device, scaler):
    """ Trains the model for one epoch using mixed precision. """
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []

    progress_bar = tqdm(dataloader, desc="Training", leave=False)
    for batch in progress_bar:
        embeddings = batch['embeddings'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        # Use autocast context manager for mixed precision
        with autocast(enabled=scaler.is_enabled()):
            outputs = model(embeddings, attention_mask)
            loss = criterion(outputs, labels)

        # Scale loss, backward pass, and optimizer step
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.detach().cpu().numpy())
        all_labels.extend(labels.detach().cpu().numpy())

        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'}) 

    avg_loss = total_loss / len(dataloader) if len(dataloader) &gt; 0 else 0
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)
    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)

    return avg_loss, accuracy, f1, precision, recall
</code></pre>

    <h3>Evaluation Function</h3>
    <pre><code class="language-python">def evaluate(model, dataloader, criterion, device):
    """ Evaluates the model on a given dataset. """
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc="Evaluating", leave=False)
        for batch in progress_bar:
            embeddings = batch['embeddings'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(embeddings, attention_mask)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'}) # Use f-string

    avg_loss = total_loss / len(dataloader) if len(dataloader) &gt; 0 else 0
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)
    precision = precision_score(all_labels, all_preds,
    average='binary', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)

    return avg_loss, accuracy, f1, precision, recall, all_preds, all_labels
</code></pre>

    <h3>Training Loop Function</h3>
    <pre><code class="language-python">def training_loop(model, train_dataloader, val_dataloader, optimizer, scheduler, criterion, device, scaler, num_epochs, patience=3, 
        model_save_path='best_model_checkpoint.pth'):
    """ Main training loop with validation, early stopping and model saving. """
    best_val_f1 = -1.0 # Initialize with a value lower than any possible F1
    epochs_no_improve = 0
    train_history = [] # Store metrics per epoch
    val_history = []

    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")

        # Train
        # Unpack the 5 returned values from train_one_epoch
        train_loss, train_acc, train_f1, train_prec, train_rec = train_one_epoch(
            model, train_dataloader, optimizer, criterion, device, scaler
        )
        # Store all returned metrics
        train_metrics = (train_loss, train_acc, train_f1, train_prec, train_rec)
        # Updated print statement to match returned values
        print(f" Train -&gt; Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}")
        train_history.append(train_metrics)

        # Evaluate
        # Unpack the 7 returned values from evaluate
        val_loss, val_acc, val_f1, val_prec, val_rec, \
        _, _ = evaluate( # Use _ for preds/labels if not needed here
            model, val_dataloader, criterion, device
        )
        # Store the metrics part
        val_metrics = (val_loss, val_acc, val_f1, val_prec, val_rec)
        # Updated print statement
        print(f" Valid -&gt; Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}")
        val_history.append(val_metrics) # Append the tuple of metrics

        # Step the scheduler (after validation)
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        print(f"  Current LR: {current_lr:.2e}")

        # Check for improvement and save best model based
        # on validation F1
        if val_f1 &gt; best_val_f1:
            print(f" Validation F1 improved ({best_val_f1:.4f} -&gt; {val_f1:.4f}). Saving model...")
            best_val_f1 = val_f1
            epochs_no_improve = 0
            # Save more info in checkpoint
            torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'loss': val_loss, # Save validation loss
            'f1': val_f1,     # Save validation F1 (the criterion)
            'accuracy': val_acc # Save validation accuracy
            }, model_save_path)
        else:
            epochs_no_improve += 1
            print(f"  Validation F1 did not improve. Patience: {epochs_no_improve}/{patience}")

        # Early stopping
        if epochs_no_improve &gt;= patience:
            print(f"\nEarly stopping triggered after {epoch+1} epochs.")
            break

    print("\nTraining complete!")
    # Load the best model state after training finishes or stops early
    if os.path.exists(model_save_path):
        print(f"Loading best model from {model_save_path}")
        # Ensure model is loaded to the correct device
        checkpoint = torch.load(model_save_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        # Use .get() for safety in case keys are missing in older checkpoints
        print(f"Best model loaded (Epoch {checkpoint.get('epoch', 'N/A')+1}, F1: {checkpoint.get('f1', -1):.4f}).")
    else:
        print(f"Warning: Best model checkpoint file '{model_save_path}' not found after training.")

    # Return the loaded best model and history
    return model, train_history, val_history
</code></pre>

    <h3>Execute Training Loop</h3>
    <pre><code class="language-python">NUM_EPOCHS = 10
PATIENCE = 3
MODEL_SAVE_PATH = 'best_model_checkpoint.pth'

if isinstance(scheduler, CosineAnnealingLR):
    # Reinitialize scheduler if T_max needs
    # to match NUM_EPOCHS for a full cycle per training run
    optimizer, scheduler = setup_optimizer_and_scheduler(
        model,
        learning_rate=LEARNING_RATE,
        t_max=NUM_EPOCHS # Set T_max to number of epochs
    )
    print(f"Re-initialized CosineAnnealingLR Scheduler with T_max={NUM_EPOCHS}")


# Start training
model, train_history, val_history = training_loop(
    model,
    train_dataloader,
    val_dataloader,
    optimizer,
    scheduler,
    criterion,
    device,
    scaler,
    num_epochs=NUM_EPOCHS,
    patience=PATIENCE,
    model_save_path=MODEL_SAVE_PATH
)
</code></pre>

    <h3>Evaluate on Combined Test Set</h3>
    <pre><code class="language-python"># Ensure the best model is loaded before final evaluation
if os.path.exists(MODEL_SAVE_PATH):
    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded best model from {MODEL_SAVE_PATH} for final test evaluation.")
else:
    print("Warning: Evaluating the model state at the end of training, not necessarily the best checkpoint.")

# Evaluate on the combined test set
# Unpack the 7 values returned by evaluate
comb_loss, comb_acc, comb_f1, comb_prec, comb_rec, comb_preds, comb_labels = evaluate(model, combined_test_dataloader, criterion, device)

# Print the results, matching the returned metrics
print("\n--- Combined Test Set Evaluation Results ---")
print(f"  Loss: {comb_loss:.4f}, Acc: {comb_acc:.4f}, F1: {comb_f1:.4f}, Prec: {comb_prec:.4f}, Rec: {comb_rec:.4f}")
</code></pre>

    <h2>ONNX Export and Validation</h2>
    <p>Exporting the trained model to the Open Neural Network Exchange (ONNX) format for broader deployment options and verifying the exported model.</p>
    <pre><code class="language-python">
if os.path.exists(MODEL_SAVE_PATH):
    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded best model from {MODEL_SAVE_PATH} for ONNX export.")
else:
    print("Warning: Exporting the model state at the end of training, not necessarily the best checkpoint.")


model.eval()  # Set model to evaluation mode for export

# Define dummy input shapes based on expected model input
BATCH_SIZE_ONNX = 1 # Using batch size 1 is often simpler for export
SEQ_LEN_ONNX = 512 # Match max_length used in dataset/tokenization or a representative length
EMBEDDING_DIM_ONNX = gte_model.config.hidden_size # Get from the base GTE model config

# Create dummy input tensors on the correct device
dummy_embeddings = torch.randn(BATCH_SIZE_ONNX, SEQ_LEN_ONNX, EMBEDDING_DIM_ONNX, device=device)
dummy_attention_mask = torch.ones(BATCH_SIZE_ONNX, SEQ_LEN_ONNX, dtype=torch.long, device=device)

# The model forward expects (embeddings, attention_mask)
dummy_input_tuple = (dummy_embeddings, dummy_attention_mask)

onnx_model_path = "best_model.onnx"
input_names = ['embeddings', 'attention_mask'] 
output_names = ['logits'] # Name of the output tensor

# Define dynamic axes for variable batch size and sequence length
dynamic_axes = {
    'embeddings': {0: 'batch_size', 1: 'sequence_length'},
    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},
    'logits': {0: 'batch_size'} # Output batch size depends on input batch size
}

# Export the model
try:
    torch.onnx.export(
        model,                  # model being run
        dummy_input_tuple,      # model input (tuple for multiple inputs)
        onnx_model_path,        # where to save the model
        export_params=True,     # store the trained parameter weights inside the model file
        opset_version=11,       # the ONNX version to export the model to (11 is common)
        do_constant_folding=True,# whether to execute constant folding for optimization
        input_names=input_names,# the model's input names
        output_names=output_names,# the model's output names
        dynamic_axes=dynamic_axes# variable length axes
    )
    print(f"Model successfully exported to ONNX format at: {onnx_model_path}")

    # --- Validate the ONNX model ---
    print("Validating the exported ONNX model...")
    onnx_model = onnx.load(onnx_model_path)
    onnx.checker.check_model(onnx_model) # Check model structure and types
    print("ONNX model is valid!")

except Exception as e:
    print(f"Error during ONNX export or validation: {e}")
</code></pre>

    <h2>Continuous Learning</h2>

    <h3>Adaptive Sparse Encoder</h3>
    <p>Defining a class to manage feature importance and apply sparsity to embeddings based on mutual information. Feature importance is updated dynamically.</p>
    <pre><code class="language-python">class AdaptiveSparseEncoder:
    """
    Manages feature importance based on mutual information
    and applies sparsity to embeddings. Adapts importance
    over time.
    """
    def __init__(self, initial_features=768, target_sparsity=0.9, adaptation_rate=0.05, device='cuda'):
        self.initial_features = initial_features
        # target_sparsity is the fraction to REMOVE (e.g., 0.9 means 90% removed)
        # keep_fraction is the fraction to KEEP (e.g., 1.0 - 0.9 = 0.1 means 10% kept)
        self.keep_fraction = 1.0 - target_sparsity
        if not (0.0 &lt;= self.keep_fraction &lt;= 1.0):
            raise ValueError("target_sparsity must be between 0.0 and 1.0")

        self.adaptation_rate = adaptation_rate
        self.device = device

        # Initialize importance and mask on the correct device
        self.feature_importance = torch.ones(initial_features, device=self.device, dtype=torch.float)
        self.feature_mask = torch.ones(initial_features, device=self.device, dtype=torch.float)
        # Store the indices of selected features (initially all)
        self.selected_features_indices = torch.arange(initial_features, device='cpu') # Store indices on CPU
        # Track history of selected indices (as numpy arrays for easier handling/saving)
        self.feature_history = []

        print(f"Initialized AdaptiveSparseEncoder: Target Sparsity={target_sparsity*100:.1f}% (Keep {self.keep_fraction*100:.1f}%), 
        Adaptation Rate={adaptation_rate}")

        # Calculate the initial mask based on initial importance (all ones)
        self.update_feature_mask() # Calculate initial mask

    def update_feature_importance(self, embeddings, labels):
        """Updates feature importance using mutual info (EMA). Expects embeddings already pooled/averaged: [batch_size, feature_dim]"""

        # Input validation
        if embeddings.dim() != 2:
            print(f"Warning: update_feature_importance received embeddings with dim {embeddings.dim()}. Expected [batch_size, feature_dim]. Skipping update.")
            return
        if embeddings.shape[0] == 0 or labels.shape[0] == 0:
            print("Warning: Cannot update feature importance with empty batch.")
            return
        if embeddings.shape[0] != labels.shape[0]:
            print(f"Warning: Embeddings batch size ({embeddings.shape[0]}) != labels batch size ({labels.shape[0]}). Skipping update.")
            return
        if embeddings.shape[1] != self.initial_features:
            print(f"Warning: Embeddings dim ({embeddings.shape[1]}) doesn't match initial_features ({self.initial_features}). Skipping update.")
            return


        with torch.no_grad(): 
            # Move data to CPU for scikit-learn compatibility
            embeddings_np = embeddings.detach().cpu().numpy()
            labels_np = labels.detach().cpu().numpy()

            try:
                # Calculate mutual information scores
                # Ensure labels are 1D array
                if labels_np.ndim &gt; 1: labels_np = labels_np.squeeze()
                mi_scores = mutual_info_classif(embeddings_np, labels_np,
                                                discrete_features=False, # Embeddings are continuous
                                                random_state=SEED) # For reproducibility if needed

                # Move scores back to the target device
                new_importance = torch.tensor(mi_scores, device=self.device, dtype=torch.float)
                # Handle potential NaNs or Infs if they occur (though unlikely with MI)
                new_importance = torch.nan_to_num(new_importance, nan=0.0, posinf=1.0, neginf=0.0)

            except ValueError as ve:
                print(f"ValueError calculating mutual information: {ve}. Ensure labels are in correct format. Skipping importance update.")
                return
            except Exception as e:
                print(f"Error calculating mutual information: {e}. Skipping importance update.")
                return

            # Apply Exponential Moving Average (EMA) update
            self.feature_importance = (
                (1.0 - self.adaptation_rate) * self.feature_importance +
                 self.adaptation_rate * new_importance
            )

            # Update the mask based on the new importance scores
            self.update_feature_mask()

    def update_feature_mask(self):
        """Updates the binary mask based on top-k feature importances."""
        # Number of features to keep
        k = int(self.initial_features * self.keep_fraction)
        # Ensure k is within valid range [1, initial_features]
        k = max(1, min(k, self.initial_features))

        # Get indices of the top-k most important features
        # Note: torch.topk returns (values, indices)
        top_k_values, top_k_indices = torch.topk(self.feature_importance, k)

        # Create the new binary mask on the correct device
        new_mask = torch.zeros_like(self.feature_mask)
        new_mask[top_k_indices] = 1.0
        self.feature_mask = new_mask

        # Store history (store sorted indices as numpy array for consistency)
        # Move indices to CPU, sort, and convert to numpy
        self.selected_features_indices = top_k_indices.cpu().sort().values
        # Append a copy of the numpy array to the history
        self.feature_history.append(self.selected_features_indices.numpy().copy())

        # print(f"Feature mask updated. Keeping top {k} features.") # Optional debug print

    def encode(self, embeddings):
        """Applies the current sparse mask to the embeddings."""
        # Adapting based on typical embedding shape [batch, seq_len, embed_dim]
        # Or pooled shape [batch, embed_dim]
        mask_shape = self.feature_mask.shape[0]
        if embeddings.shape[-1] != mask_shape:
            raise ValueError(f"Embedding dimension ({embeddings.shape[-1]}) does not match feature mask dimension ({mask_shape})")

        if embeddings.dim() == 3: # [B, T, C]
            # Expand mask: [C] -&gt; [1, 1, C] for broadcasting
            mask = self.feature_mask.view(1, 1, -1)
        elif embeddings.dim() == 2: # [B, C]
            # Expand mask: [C] -&gt; [1, C] for broadcasting
            mask = self.feature_mask.view(1, -1)
        else:
            mask = self.feature_mask
        # Apply mask element-wise
        return embeddings * mask

    def get_active_features(self):
        """Returns the indices of currently active (non-zero mask) features."""
        # Find indices where mask is &gt; 0, squeeze to make it 1D
        return torch.nonzero(self.feature_mask &gt; 0).squeeze()

    def analyze_feature_adaptation(self):
        """Analyzes how features have adapted over time using Jaccard index."""
        if len(self.feature_history) &lt;= 1:
            print("Not enough history (&lt;= 1 entry) to analyze feature adaptation.")
            return None

        stabilities = []
        # Compare consecutive sets of selected features
        for i in range(len(self.feature_history) - 1):
            set1 = set(self.feature_history[i])
            set2 = set(self.feature_history[i+1])
            intersection = len(set1.intersection(set2))
            union = len(set1.union(set2))

            # Calculate Jaccard index (stability)
            if union &gt; 0:
                stability = intersection / union
                stabilities.append(stability)
            elif len(set1) == 0 and len(set2) == 0: # Both empty
                stabilities.append(1.0) # Define stability as 1 if sets remain empty
            else: # One is empty, one is not, or union is 0 unexpectedly
                stabilities.append(0.0) # Define stability as 0

        # Calculate mean stability (handle empty list case)
        mean_stability = np.mean(stabilities) if stabilities else 0.0

        # Get current active features and their importance stats
        active_indices = self.get_active_features() # Get indices on device
        current_importance = self.feature_importance[active_indices]

        # Calculate mean/std safely, handle case with 0 or 1 active features
        mean_importance = float(current_importance.mean().item()) if current_importance.numel() &gt; 0 else 0.0
        std_importance = float(current_importance.std().item()) if current_importance.numel() &gt; 1 else 0.0 # Std needs &gt; 1 element

        return {
            'num_history_points': len(self.feature_history),
            'feature_stability_jaccard': mean_stability,
            'active_features_count': int(self.feature_mask.sum().item()),
            # Return indices as a list of integers (moved to CPU)
            'active_indices': active_indices.cpu().numpy().tolist(),
            'importance_stats_active': {
                'mean': mean_importance,
                'std': std_importance
            }
        }
</code></pre>

    <h3>Keyword Extraction and Fact-Checking Utilities</h3>
    <p>Functions to extract keywords from text (removing common stopwords) and fetch recent news headlines or fact-checks using external APIs.</p>
    <pre><code class="language-python"># --- Stopwords ---
# Basic Arabic Stopwords (expand significantly for real use)
arabic_stopwords = {
    "في", "من", "على", "الى", "عن", "هو", "هي", "كان", "يكون", "كل", "هذا",
    "هذه", "ذلك", "تلك", "ان", "او", "مع", "قد", "لم", "لن", "لا", "ما",
    "ماذا", "كيف", "متى", "اين", "ب", "ل", "و", "ثم", "حتى", "مثل", "غير",
    "له", "لها", "به", "بها", "فيه", "فيها", "عليه", "عليها", "إلى", "تم",
    "يكون", "تكون", "أنه", "أنها", "وقال", "وقالت", "وذكر", "وذكرت",
    "الذي", "التي", "الذين", "اللاتي", "اللائي",
    "the", "a", "an", "is", "are", "was", "were", "in", "on", "at", "to", "for", "of",
    "and", "or", "but", "with", "as", "by", "this", "that", "it", "he", "she",
    "they", "we", "you", "i", "has", "have", "had", "will", "would", "can", "could",
    "said", "report", "reports", "study", "says"
}


def extract_keywords(text, min_len=4): # Increased min_len slightly
    """Extracts potential keywords by removing stopwords and short words."""
    if not isinstance(text, str):
        return []
    # Regex to find words (handles Arabic and Latin characters)
    # \w includes underscores, might want [a-zA-Z0-9\u0600-\u06FF]+ for letters/numbers only
    words = re.findall(r'\b\w+\b', text.lower())
    keywords = [word for word in words if
                word not in arabic_stopwords and len(word) &gt;= min_len and not word.isdigit()]
    return keywords

# --- API Keys (Replace with your actual keys or load from environment) ---
SERPER_API_KEY = os.environ.get("SERPER_API_KEY", "YOUR_SERPER_API_KEY")
FACTCHECK_API_KEY = os.environ.get("FACTCHECK_API_KEY", "YOUR_FACTCHECK_API_KEY")


def get_recent_keywords(num_keywords=15, query="latest news OR آخر الأخبار"): # Broadened query
    """Fetches recent news using Serper and extracts common keywords."""
    if SERPER_API_KEY == "YOUR_SERPER_API_KEY" or not SERPER_API_KEY:
        print("Warning: SERPER_API_KEY not set or invalid. Cannot fetch recent news.")
        return []

    url = "https://google.serper.dev/news"
    payload = json.dumps({"q": query, "gl": "us", "hl": "en"}) # Example: Search US/English news
    headers = {
        'X-API-KEY': SERPER_API_KEY,
        'Content-Type': 'application/json'
    }
    try:
        response = requests.post(url, headers=headers, data=payload, timeout=15) # Increased timeout
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        response_data = response.json()
    except requests.exceptions.Timeout:
        print("Error: Timeout occurred while fetching recent news from Serper.")
        return []
    except requests.exceptions.RequestException as e:
        print(f"Error fetching recent news from Serper: {e}")
        return []
    except json.JSONDecodeError:
        print("Error decoding Serper API JSON response.")
        return []

    all_keywords = []
    news_items = response_data.get('news', [])
    if not news_items:
        print("No news items returned by Serper API.")
        return []

    for item in news_items:
        title = item.get('title', '')
        snippet = item.get('snippet', '')
        # Combine title and snippet for keyword extraction
        text_content = (title or '') + ' ' + (snippet or '')
        all_keywords.extend(extract_keywords(text_content))

    if not all_keywords:
        print("No keywords extracted from the fetched recent news titles/snippets.")
        return []

    # Use Counter to find the most frequent keywords
    keyword_counts = Counter(all_keywords)
    most_common_keywords = [keyword for keyword, count
                            in keyword_counts.most_common(num_keywords)]

    print(f"Top {len(most_common_keywords)} keywords from recent news: {most_common_keywords}")
    return most_common_keywords


def get_fact_checked_news(keywords):
    """Fetches fact-checks related to given keywords using Google Fact Check API."""
    if not keywords:
        print("No keywords provided to fetch fact-checks.")
        return []
    if FACTCHECK_API_KEY == "YOUR_FACTCHECK_API_KEY" or not FACTCHECK_API_KEY:
        print("Warning: FACTCHECK_API_KEY not set or invalid. Cannot fetch fact-checks.")
        return []

    results = []
    print(f"Fetching fact-checks for {len(keywords)} keywords...")
    processed_urls = set() # To avoid duplicate entries from same URL

    for query in keywords:
        try:
            # Encode query for URL safety
            encoded_query = requests.utils.quote(query)
            # Construct API URL, optionally add language constraint if desired
            # e.g., &amp;languageCode=en or &amp;languageCode=ar
            url = (f'https://factchecktools.googleapis.com/v1alpha1/claims:search?query={encoded_query}&amp;key={FACTCHECK_API_KEY}')

            response = requests.get(url, timeout=15) # Increased timeout
            response.raise_for_status() # Check for HTTP errors
            data = response.json()

            # Process claims if they exist
            if 'claims' in data and data['claims']:
                for claim in data['claims']:
                    text = claim.get('text')
                    # claim_date = claim.get('claimDate')

                    # Iterate through claim reviews (the actual fact-checks)
                    if text and 'claimReview' in claim: # Ensure claim text exists
                        for review in claim['claimReview']:
                            review_url = review.get('url')
                            # Skip if we already processed this URL
                            if not review_url or review_url in processed_urls:
                                continue

                            publisher = review.get('publisher', {}).get('name')
                            rating = review.get('textualRating', '').lower() # Standardize to lower
                            lang = review.get('languageCode', '').lower()

                            # Filter for relevant languages and non-empty ratings
                            if (lang == 'ar' or lang == 'en') and rating:
                                results.append({
                                'claim_text': text.strip(),
                                'rating': rating,
                                'review_url': review_url,
                                'publisher': publisher or 'Unknown Publisher',
                                'language': lang,
                                'query_keyword': query # Keep track of which keyword found it
                                })
                                processed_urls.add(review_url) # Mark URL as processed

        except requests.exceptions.Timeout:
            print(f"Timeout occurred while fetching fact-check for '{query}'.")
        except requests.exceptions.RequestException as e:
            # Handle potential API errors (e.g., quota exceeded, invalid key)
            print(f"API Error fetching fact-check for '{query}': {e}")
            if response: print(f"Response text: {response.text[:200]}...") # Log part of error response
        except json.JSONDecodeError:
            print(f"Error decoding Fact Check API JSON response for '{query}'.")
        except Exception as e: # Catch any other unexpected errors
            print(f"Unexpected error during fact-check processing for '{query}': {e}")


    print(f"Found {len(results)} relevant fact-checks.")
    return results
</code></pre>

    <h3>Modified News Dataset (for Continuous Learning)</h3>
    <p>A Dataset class specifically for loading new text data during the continuous learning phase. It assumes tokenization and embedding will happen within the training/update loop.</p>
    <pre><code class="language-python">class NewsUpdateDataset(Dataset):
    """ Dataset for loading new news articles for model updates.
        Does not perform embedding here; assumes it happens in the update loop.
        Handles basic tokenization for EWC Fisher calculation if needed.
    """
    def __init__(self, texts, labels, tokenizer, max_length=512):
        # Ensure texts and labels are indexable (list, Series, numpy array)
        self.texts = list(texts) if not isinstance(texts, list) else texts
        self.labels = list(labels) if not isinstance(labels, list) else labels
        self.tokenizer = tokenizer
        self.max_length = max_length

        if len(self.texts) != len(self.labels):
            raise ValueError(f"Texts and labels must have the same length ({len(self.texts)} != {len(self.labels)}).")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx]) # Ensure text is string
        label = self.labels[idx]

        # Tokenize the text - usually done here for EWC Fisher calculation later
        # Padding='max_length' is common here if subsequent steps expect fixed size
        inputs = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length', # Pad to max_length for consistent input to GTE if needed by EWC
            truncation=True,
            return_tensors='pt' # Return PyTorch tensors
        )

        # Squeeze to remove the batch dimension added by tokenizer
        input_ids = inputs['input_ids'].squeeze(0)
        attention_mask = inputs['attention_mask'].squeeze(0)

        return {
            'input_ids': input_ids,       # Used for GTE embedding in update loop/EWC
            'attention_mask': attention_mask, # Used for GTE embedding and model forward pass
            'labels': torch.tensor(label, dtype=torch.long) # Ensure label is tensor
        }
</code></pre>

    <h2>Adaptive Sparse Model</h2>
    <p>Modifying the main model architecture to incorporate the <code>AdaptiveSparseEncoder</code>. The forward pass now optionally updates feature importance if training and labels are provided, and applies the sparse encoding.</p>
    <pre><code class="language-python">class AdaptiveSparseModel(nn.Module):
    """ CombinedModel modified to include AdaptiveSparseEncoder. """
    def __init__(self, embedding_dim=768,
                hidden_dim_ffn_multiplier=4, nhead=8,
                num_classes=2,
                num_encoder_layers=6, dropout=0.1,
                target_sparsity=0.9, # Default sparsity target
                adaptation_rate=0.05, # Default adaptation rate
                device='cuda'):
        super().__init__()
        self.embedding_dim = embedding_dim
        dim_feedforward = embedding_dim * hidden_dim_ffn_multiplier

        # Core Transformer components
        self.pos_encoder = FourierPositionalEncoding(embedding_dim, max_len=1024)
        self.encoder = Encoder(embedding_dim, nhead, dim_feedforward, num_encoder_layers, dropout)
        self.classifier = ClassifierHead(embedding_dim, num_classes, dropout_rate=dropout)

        # Adaptive Sparsity Component
        self.sparse_encoder = AdaptiveSparseEncoder(
            initial_features=embedding_dim,
            target_sparsity=target_sparsity,
            adaptation_rate=adaptation_rate,
            device=device # Pass device to the sparse encoder
        )
        print(f"AdaptiveSparseModel initialized with sparsity target {target_sparsity}")

    def forward(self, embeddings, attention_mask,
                labels=None, training=False):
        """
        Forward pass. If training=True and labels are provided,
        updates feature importance before applying sparsity.
        """
        # Ensure embeddings are on the same device as the model
        embeddings = embeddings.to(next(self.parameters()).device)
        attention_mask = attention_mask.to(next(self.parameters()).device)

        # --- Update Feature Importance (only during training with labels) ---
        if training and labels is not None:
            # Pool embeddings before calculating importance
            # Use the same pooling strategy as in the main forward pass
            with torch.no_grad(): # Don't track gradients for importance calculation input
                mask_expanded_pool = attention_mask.unsqueeze(-1).float()
                masked_embeddings = embeddings * mask_expanded_pool
                sum_embeddings_for_mi = masked_embeddings.sum(dim=1)
                lengths_for_mi = attention_mask.sum(dim=1).unsqueeze(-1).float().clamp(min=1e-6)
                pooled_embeddings_for_mi = sum_embeddings_for_mi / lengths_for_mi

                # Ensure labels are on the same device as pooled embeddings
                labels = labels.to(pooled_embeddings_for_mi.device)

                # Update importance if pooled embeddings are valid
                if pooled_embeddings_for_mi.shape[0] &gt; 0:
                    self.sparse_encoder.update_feature_importance(pooled_embeddings_for_mi, labels)
                else:
                    print("Skipping feature importance update due to empty effective batch after pooling.")
        # --- Apply Sparsity ---
        # Apply the current mask from the sparse encoder
        try:
            sparse_embeddings = self.sparse_encoder.encode(embeddings)
        except ValueError as e:
            print(f"Error applying sparse encoding: {e}. Using original embeddings.")
            sparse_embeddings = embeddings # Fallback


        # --- Standard Transformer Forward Pass ---
        x = self.pos_encoder(sparse_embeddings) # Use sparse embeddings
        src_key_padding_mask = (attention_mask == 0) # Mask where attention is 0
        encoder_output = self.encoder(x, src_key_padding_mask=src_key_padding_mask)

        # --- Pooling for Classifier ---
        # Use the same masked average pooling as before
        mask_expanded_pool_clf = attention_mask.unsqueeze(-1).float()
        masked_encoder_output = encoder_output * mask_expanded_pool_clf
        sum_embeddings_pool = masked_encoder_output.sum(dim=1)
        lengths_pool = attention_mask.sum(dim=1).unsqueeze(-1).float().clamp(min=1e-6)
        pooled_output = sum_embeddings_pool / lengths_pool

        # --- Classification ---
        logits = self.classifier(pooled_output)

        return logits

    def get_sparsity_info(self):
        """ Utility method to get adaptation stats from the sparse encoder. """
        if hasattr(self, 'sparse_encoder') and hasattr(self.sparse_encoder, 'analyze_feature_adaptation'):
            return self.sparse_encoder.analyze_feature_adaptation()
        else:
            return None

# --- Instantiate the Adaptive Model ---
# Example instantiation parameters (adjust as needed)
ADAPTIVE_SPARSITY_TARGET = 0.8 # e.g., keep 20% of features
ADAPTIVE_RATE = 0.05
adaptive_model = AdaptiveSparseModel(
    embedding_dim=gte_model.config.hidden_size,
    num_encoder_layers=6,
    nhead=8,
    hidden_dim_ffn_multiplier=4,
    num_classes=2,
    target_sparsity=ADAPTIVE_SPARSITY_TARGET,
    adaptation_rate=ADAPTIVE_RATE,
    device=device # Ensure device is passed correctly
)
adaptive_model.to(device) # Move model to the target device

# --- Load Weights from Initial Training (Important!) ---
# Load the state dict from the best initially trained `CombinedModel`.
if os.path.exists(MODEL_SAVE_PATH):
    print(f"Loading weights from '{MODEL_SAVE_PATH}' into AdaptiveSparseModel...")
    try:
        checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)
        missing_keys, unexpected_keys = adaptive_model.load_state_dict(
            checkpoint['model_state_dict'], strict=False
        )
        print("Weights loaded.")
        if missing_keys:
            print(f"  Missing keys (expected in adaptive model but not in checkpoint): {missing_keys}")
        if unexpected_keys:
            print(f"  Unexpected keys (in checkpoint but not adaptive model): {unexpected_keys}")
    except Exception as e:
        print(f"Error loading weights into AdaptiveSparseModel: {e}. Starting with initialized weights.")

else:
    print(f"Warning: No pre-trained checkpoint found at '{MODEL_SAVE_PATH}' AdaptiveSparseModel starts with random weights (except loaded base GTE).")


# --- Setup Optimizer for the Adaptive Model ---
# Use a potentially smaller learning rate for fine-tuning/adaptation
ADAPTIVE_LR = 1e-5
ADAPTIVE_SCHEDULER_T_MAX = 50 # Example scheduler length for adaptation phase
adaptive_optimizer, adaptive_scheduler = setup_optimizer_and_scheduler(
    adaptive_model, # Use the adaptive model instance
    learning_rate=ADAPTIVE_LR,
    t_max=ADAPTIVE_SCHEDULER_T_MAX
)
print(f"Optimizer and Scheduler set up for AdaptiveSparseModel with LR={ADAPTIVE_LR}")
</code></pre>

    <h3>Elastic Weight Consolidation (EWC)</h3>
    <p>Implementing the EWC algorithm to mitigate catastrophic forgetting during continuous learning by penalizing changes to parameters important for previous tasks.</p>
    <pre><code class="language-python">class EWC:
    """ Elastic Weight Consolidation Implementation. """
    def __init__(self, model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device, gte_model: nn.Module):
        """
        Args:
            model: The model being trained (e.g., AdaptiveSparseModel).
            dataloader: DataLoader for the data representing the *previous* task.
                        Used to compute Fisher information.
            criterion: The loss function used for the task.
            device: The device ('cuda' or 'cpu').
            gte_model: The base embedding model (GTE) used to get embeddings.
        """
        self.model = model
        self.dataloader = dataloader
        self.criterion = criterion
        self.device = device
        self.gte_model = gte_model # Store the GTE model
        self.fisher_matrix = None  # Dictionary to store Fisher info diagonal
        self.optimal_params = None # Dictionary to store optimal parameters from previous task
        print("EWC Initialized. Compute Fisher information before applying loss penalty.")


    def compute_fisher_information(self):
        """Computes the diagonal of the Fisher Information Matrix for the *current*
           model state over the *previous* task's data (provided in dataloader)."""
        print("Computing Fisher Information Matrix...")
        if self.dataloader is None:
            print("Error: Dataloader for previous task not provided. Cannot compute Fisher.")
            return

        self.model.eval() # Set model to evaluation mode
        self.gte_model.eval() # Ensure GTE model is also in eval mode

        # Initialize Fisher matrix with zeros, for parameters requiring gradients
        self.fisher_matrix = {
            name: torch.zeros_like(param.data)
            for name, param in self.model.named_parameters()
            if param.requires_grad
        }

        num_samples = 0
        progress_bar = tqdm(self.dataloader, desc="Computing Fisher Info", leave=False)
        for batch in progress_bar:
            self.model.zero_grad() # Clear gradients before processing batch

            # --- Get Embeddings ---
            # Check if the dataloader provides 'input_ids' or 'embeddings'
            if 'input_ids' in batch:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                # Generate embeddings using the base GTE model
                with torch.no_grad(): # No grad needed for GTE pass
                    gte_outputs = self.gte_model(input_ids=input_ids, attention_mask=attention_mask)
                    embeddings = gte_outputs.last_hidden_state # Shape: [B, T, C]
            elif 'embeddings' in batch:
                # If dataloader directly provides embeddings (e.g., from initial training set)
                embeddings = batch['embeddings'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
            else:
                print("Error: Batch does not contain 'input_ids' or 'embeddings'. Cannot compute Fisher.")
                continue # Skip batch

            labels = batch['labels'].to(self.device)
            batch_size = labels.size(0)
            num_samples += batch_size

            # --- Forward Pass &amp; Loss ---
            # Run forward pass through the *main* model (the one being regularized)
            # Pass training=False as we are evaluating likelihood on old data
            outputs = self.model(embeddings, attention_mask, training=False)
            # Calculate loss using the provided criterion (needed for some Fisher approximations)
            loss = self.criterion(outputs, labels)

            # --- Fisher Calculation (using squared gradients of loss) ---
            # loss.backward() # Calculate gradients of loss w.r.t. parameters

            # --- Alternative: Fisher using gradients of log-likelihood (more standard) ---
            log_probs = F.log_softmax(outputs, dim=1)
            # Get log-probability of the true class for each sample
            log_likelihood_samples = log_probs.gather(1, labels.unsqueeze(1)).squeeze()
            # Average log-likelihood over the batch (or sum, doesn't matter for gradient direction)
            log_likelihood = log_likelihood_samples.mean()
            # Calculate gradients of the log-likelihood w.r.t. parameters
            # Use retain_graph=True if this graph is needed later (usually not for Fisher)
            log_likelihood.backward()


            # --- Accumulate Squared Gradients ---
            for name, param in self.model.named_parameters():
                if param.requires_grad and param.grad is not None:
                    if name in self.fisher_matrix:
                        # Accumulate squared gradients
                        self.fisher_matrix[name] += (param.grad.data ** 2)
                    # else: # This should not happen if initialized correctly
                    #     print(f"Warning: Param '{name}' not found in initialized Fisher matrix.")

            # Clear gradients for the next batch
            self.model.zero_grad()


        # --- Normalize Fisher Matrix ---
        if num_samples &gt; 0:
            for name in self.fisher_matrix:
                self.fisher_matrix[name] /= num_samples
            print(f"Fisher Information Matrix computed and normalized over {num_samples} samples for {len(self.fisher_matrix)} parameters.")
        else:
            print("Warning: No samples processed. Fisher Information Matrix is empty or zero.")


    def save_optimal_params(self):
        """Saves a copy of the current model parameters (should be called *after* training on a task is complete, before starting the next)."""
        self.optimal_params = {
            name: param.clone().detach() # Create a detached copy
            for name, param in self.model.named_parameters()
            if param.requires_grad
        }
        print("Optimal parameters saved for EWC regularization.")

    def ewc_loss(self, lambda_ewc=1.0):
        """Calculates the EWC penalty term."""
        if self.fisher_matrix is None or self.optimal_params is None:
            # Return zero loss if EWC hasn't been initialized properly
            return torch.tensor(0.0, device=self.device)

        penalty = 0.0
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.fisher_matrix:
                fisher = self.fisher_matrix[name]    # Fisher diagonal for this param
                optimal = self.optimal_params[name] # Optimal param value from previous task

                # Ensure dimensions match (should always if computed correctly)
                if fisher.shape == param.shape and optimal.shape == param.shape:
                    # EWC penalty: 0.5 * lambda * sum(Fisher * (param - optimal_param)^2)
                    penalty += (fisher * (param - optimal) ** 2).sum()
                else:
                    print(f"Warning: Shape mismatch for EWC param '{name}'. Param: {param.shape}, Fisher: {fisher.shape}, Optimal: {optimal.shape}. Skipping.")

        # The EWC loss is (lambda / 2) * penalty
        return (lambda_ewc / 2.0) * penalty
</code></pre>

    <h2>Continuous Learning Pipeline</h2>
    <p>Defining a pipeline class to manage the overall continuous learning process: loading checkpoints, updating the model with new data, applying EWC, handling feature adaptation, and saving progress.</p>
    <pre><code class="language-python">class ContinuousLearningPipeline:
    """
    Manages the continuous learning process including model updates, EWC, feature adaptation, and checkpointing.
    """
    def __init__(self, model_class=AdaptiveSparseModel, # The class of the model to use
                base_model_path='best_model_checkpoint.pth', # Initial weights
                save_dir='model_checkpoints_continuous', # Where to save updates
                ewc_lambda=5000.0, # EWC regularization strength
                learning_rate=1e-5, # LR for updates
                update_batch_size=16, # Batch size for update data
                device='cuda',
                tokenizer=byt5_tokenizer, # Tokenizer instance
                embedding_model=gte_model, # Base embedding model instance
                ewc_dataloader_source='initial_val' # 'initial_val' or 'previous_update'
                ):

        self.device = device
        self.model_class = model_class
        self.tokenizer = tokenizer
        self.embedding_model = embedding_model.to(self.device).eval() # Ensure GTE is on device and eval mode
        self.save_dir = save_dir
        self.ewc_lambda = ewc_lambda
        self.lr = learning_rate
        self.update_batch_size = update_batch_size
        self.ewc_dataloader_source = ewc_dataloader_source # How to get data for Fisher calc

        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            print(f"Created checkpoint directory: {save_dir}")

        # --- Initialize Model ---
        # Instantiate the specified model class with necessary args
        # Assuming AdaptiveSparseModel takes these args - adjust if different
        self.model = self.model_class(
            embedding_dim=self.embedding_model.config.hidden_size, device=self.device).to(self.device)

        # --- Setup Optimizer and Loss ---
        # Optimizer will be re-initialized or loaded from checkpoint
        self.optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)
        self.criterion = nn.CrossEntropyLoss()

        # --- State Tracking ---
        self.best_performance = float('inf') # Use loss as performance metric (lower is better)
        self.current_update_cycle = 0
        # Placeholder for EWC object, initialized when first update occurs
        self.ewc: EWC | None = None
        # Placeholder for the dataloader used for the *last* Fisher calculation
        self.ewc_reference_dataloader: DataLoader | None = None

        # --- Load Initial State ---
        # Load the *absolute* base model weights first
        self.load_checkpoint(model_path=base_model_path, is_initial_load=True)
        # Then try loading the *latest continuous* checkpoint if one exists
        latest_checkpoint = self.find_latest_checkpoint()
        if latest_checkpoint and latest_checkpoint != base_model_path:
            print(f"\nAttempting to load latest continuous checkpoint: {latest_checkpoint}")
            self.load_checkpoint(model_path=latest_checkpoint, is_initial_load=False)
        else:
            print("\nStarting continuous learning from the base model checkpoint.")
            # If starting fresh, we need a dataset to compute the first Fisher matrix
            self._prepare_initial_ewc_reference_data()


    def find_latest_checkpoint(self):
        """Finds the most recent checkpoint file in the save directory."""
        try:
            checkpoints = [f for f in os.listdir(self.save_dir) if f.startswith('checkpoint_') and f.endswith('.pth')]
            if not checkpoints:
                # Check for the 'best_model_checkpoint.pth' as a fallback
                best_ckpt_path = os.path.join(self.save_dir, 'best_model_checkpoint.pth')
                if os.path.exists(best_ckpt_path):
                    return best_ckpt_path
                return None

            # Sort by timestamp (assuming YYYYMMDD_HHMMSS format)
            checkpoints.sort(reverse=True)
            return os.path.join(self.save_dir, checkpoints[0])
        except FileNotFoundError:
            return None
        except Exception as e:
            print(f"Error finding latest checkpoint: {e}")
            return None

    def _prepare_initial_ewc_reference_data(self):
        """ Prepares the dataloader for the first EWC Fisher calculation.
            Uses the initial validation set. """
        global val_dataloader # Access the globally defined initial validation loader
        if 'val_dataloader' in globals() and val_dataloader is not None:
            print("Using initial validation dataloader for the first EWC Fisher calculation.")
            # We need to reconstruct it using NewsUpdateDataset format if EWC expects 'input_ids'
            # This requires access to the original validation dataset split logic, which might be complex here.
            # Simplification: Assume EWC can handle the 'embeddings' format from the original val_dataloader
            self.ewc_reference_dataloader = val_dataloader
        else:
            print("Warning: Initial validation dataloader ('val_dataloader') not found. "Cannot prepare initial EWC reference data.")
            self.ewc_reference_dataloader = None


    def load_checkpoint(self, model_path, is_initial_load=False):
        """Loads model, optimizer, performance, EWC state, etc."""
        if not model_path or not os.path.exists(model_path):
            print(f"Checkpoint path '{model_path}' not found or not provided.")
            if is_initial_load:
                print("Warning: Could not load initial base model. Starting with random weights.")
            return # Don't proceed if checkpoint doesn't exist

        print(f"Loading checkpoint from: {model_path}")
        try:
            checkpoint = torch.load(model_path, map_location=self.device)

            # --- Load Model State ---
            if 'model_state_dict' in checkpoint:
                missing_keys, unexpected_keys = self.model.load_state_dict(
                    checkpoint['model_state_dict'], strict=False # Allow partial loads
                )
                print("Model state loaded.")
                if missing_keys: print(f"  - Missing keys: {missing_keys}")
                if unexpected_keys: print(f"  - Unexpected keys: {unexpected_keys}")
            else:
                print("Warning: Checkpoint does not contain 'model_state_dict'.")


            # --- Load Optimizer State (only if not initial base model load) ---
            if not is_initial_load and 'optimizer_state_dict' in checkpoint and hasattr(self, 'optimizer'):
                try:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("Optimizer state loaded.")
                except Exception as e:
                    print(f"Could not load optimizer state (may need re-initialization): {e}. Reinitializing.")
                    self.optimizer = optim.AdamW(self.model.parameters(), lr=self.lr) # Reinitialize
            elif not is_initial_load:
                print("Optimizer state not found in checkpoint or optimizer not yet initialized.")
                # Ensure optimizer is initialized if loading a continuous checkpoint without opt state
                if not hasattr(self, 'optimizer') or self.optimizer is None:
                    self.optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)


            # --- Load Performance Metric ---
            # Use the best performance from the checkpoint if available
            self.best_performance = checkpoint.get('best_performance', self.best_performance)
            print(f"Best recorded performance (loss): {self.best_performance}")

            # --- Load Sparse Encoder State ---
            if isinstance(self.model, AdaptiveSparseModel) and 'sparse_encoder_state' in checkpoint:
                try:
                    self.model.sparse_encoder.__dict__.update(checkpoint['sparse_encoder_state'])
                    # Ensure tensors within the state are on the correct device
                    for key, value in self.model.sparse_encoder.__dict__.items():
                        if isinstance(value, torch.Tensor):
                            self.model.sparse_encoder.__dict__[key] = value.to(self.device)
                    print("Sparse encoder state loaded and tensors moved to device.")
                except Exception as e:
                    print(f"Error loading sparse encoder state: {e}")


            # --- Load EWC State (Fisher Matrix and Optimal Params) ---
            # Store loaded EWC state temporarily; EWC object itself is created before first update
            if 'ewc_fisher_matrix' in checkpoint and 'ewc_optimal_params' in checkpoint:
                self._loaded_fisher = checkpoint['ewc_fisher_matrix']
                self._loaded_optimal_params = checkpoint['ewc_optimal_params']
                # Ensure EWC tensors are on the correct device
                for name in self._loaded_fisher:
                    if isinstance(self._loaded_fisher[name], torch.Tensor):
                        self._loaded_fisher[name] = self._loaded_fisher[name].to(self.device)
                for name in self._loaded_optimal_params:
                    if isinstance(self._loaded_optimal_params[name], torch.Tensor):
                        self._loaded_optimal_params[name] = self._loaded_optimal_params[name].to(self.device)

                print("Loaded EWC Fisher matrix and optimal params (will be used by EWC object).")
            else:
                # Clear any previously loadd EWC state if not in current checkpoint
                if hasattr(self, '_loaded_fisher'): del self._loaded_fisher
                if hasattr(self, '_loaded_optimal_params'): del self._loaded_optimal_params


            # Update cycle count if available in checkpoint
            self.current_update_cycle = checkpoint.get('update_cycle', self.current_update_cycle)
            print(f"Checkpoint loaded successfully. Current update cycle: {self.current_update_cycle}")


        except FileNotFoundError:
            print(f"Error: Checkpoint file not found at {model_path}.")
        except Exception as e:
            print(f"Error loading checkpoint from {model_path}: {e}")
            # import traceback
            # print(traceback.format_exc()) # For detailed debugging
            if is_initial_load:
                print("Failed to load base model, starting fresh.")
                self.best_performance = float('inf') # Reset performance


    def save_checkpoint(self, performance_metric, is_best=False):
        """Saves model checkpoint with timestamp, performance, and relevant states."""
        self.current_update_cycle += 1 # Increment cycle count on save
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        checkpoint_name = f'checkpoint_{timestamp}_cycle{self.current_update_cycle}.pth'
        checkpoint_path = os.path.join(self.save_dir, checkpoint_name)

        state = {
            'timestamp': timestamp,
            'update_cycle': self.current_update_cycle,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_performance': self.best_performance, # The best loss seen so far
            'current_performance': performance_metric, # Loss from this update cycle
            'model_class': self.model.__class__.__name__, # Store model class name
            # Store hyperparameters used for this cycle if needed
            'ewc_lambda': self.ewc_lambda,
            'learning_rate': self.lr,
        }

        # Save Sparse Encoder state if applicable
        if isinstance(self.model, AdaptiveSparseModel):
            sparse_state = {
                # Save tensors to CPU to avoid device issues on loading
                'feature_importance': self.model.sparse_encoder.feature_importance.cpu(),
                'feature_mask': self.model.sparse_encoder.feature_mask.cpu(),
                'feature_history': self.model.sparse_encoder.feature_history # History is already numpy/list
                # Add other relevant sparse encoder attributes if needed
            }
            state['sparse_encoder_state'] = sparse_state

        # Save EWC state if EWC is active
        if self.ewc and self.ewc.fisher_matrix and self.ewc.optimal_params:
            # Save tensors to CPU
            state['ewc_fisher_matrix'] = {k: v.cpu() for k, v in self.ewc.fisher_matrix.items()}
            state['ewc_optimal_params] = {k: v.cpu() for k, v in self.ewc.optimal_params.items()}


        try:
            torch.save(state, checkpoint_path)
            print(f"Saved checkpoint to {checkpoint_path} (Cycle {self.current_update_cycle}, Perf: {performance_metric:.4f})")

            # If this is the best performance seen, also save as 'best_model_checkpoint.pth'
            if is_best:
                best_model_path = os.path.join(self.save_dir, 'best_model_checkpoint.pth')
                torch.save(state, best_model_path)
                print(f"Saved **best** model checkpoint to {best_model_path}")
        except Exception as e:
            print(f"Error saving checkpoint: {e}")


    def _prepare_update_dataloader(self, new_texts, new_labels):
        """Creates a DataLoader for the new data using NewsUpdateDataset."""
        try:
            dataset = NewsUpdateDataset(new_texts, new_labels, self.tokenizer, max_length=self.model.pos_encoder.pe.size(1)) # Use model's max_len
            # Use pin_memory if using CUDA
            pin_memory = (self.device.type == 'cuda')
            dataloader = DataLoader(dataset,
                                    batch_size=self.update_batch_size,
                                    shuffle=True,
                                    num_workers=2, # Adjust based on system
                                    pin_memory=pin_memory)
            print(f"Created DataLoader for update data with {len(dataset)} samples.")
            return dataloader
        except Exception as e:
            print(f"Error creting update dataloader: {e}")
            return None


    def update_model(self, new_texts, new_labels):
        """Updates the model with new data, applies EWC, saves checkpoints."""
        if not new_texts or not new_labels:
            print("No new data provided for update.")
            return None
        if len(new_texts) != len(new_labels):
            print("Error: Mismatch between number of new texts and labels.")
            return None

        print(f"\n--- Starting Update Cycle {self.current_update_cycle + 1} ---")
        print(f"Updating model with {len(new_texts)} new examples...")
        start_time = time.time()

        # --- Prepare DataLoader for the new update data ---
        update_dataloader = self._prepare_update_dataloader(new_texts, new_labels)
        if update_dataloader is None:
            print("Failed to create update dataloader. Skipping update cycle.")
            return None

        # --- Initialize EWC (if first update or not already initialized) ---
        if self.ewc_lambda &gt; 0 and self.ewc is None:
            print("Initializing EWC for the first time...")
            # Determine which dataloader to use for Fisher calculation
            if self.ewc_reference_dataloader:
                print("Using pre-defined reference dataloader for Fisher calculation.")
                fisher_dataloader = self.ewc_reference_dataloader
            else:
                print("Warning: No reference dataloader for EWC. Using current update data for initial Fisher calc. "
                    "This might not be ideal for measuring importance on the 'previous' task.")
                fisher_dataloader = update_dataloader # Less ideal fallback

            self.ewc = EWC(self.model, fisher_dataloader, self.criterion, self.device, self.embedding_model)

            # Check if Fisher/optimal params were loaded from a checkpoint
            if hasattr(self, '_loaded_fisher') and hasattr(self, '_loaded_optimal_params'):
                print("Using pre-loaded Fisher matrix and optimal params for EWC.")
                self.ewc.fisher_matrix = self._loaded_fisher
                self.ewc.optimal_params = self._loaded_optimal_params
                # Clear temporary attributes after loading into EWC object
                del self._loaded_fisher
                del self._loaded_optimal_params
            else:
                print("Computing initial Fisher Information Matrix...")
                self.ewc.compute_fisher_information()
                # Save the parameters *before* the first update begins
                self.ewc.save_optimal_params()
                print("Initial optimal parameters saved.")


        # --- Training Loop for Update (typically 1 epoch) ---
        self.model.train() # Set model to training mode
        total_loss = 0.0
        total_class_loss = 0.0
        total_ewc_penalty = 0.0
        num_batches = 0
        progress_bar = tqdm(update_dataloader, desc=f"Update Cycle {self.current_update_cycle + 1}", leave=False)

        for batch in progress_bar:
            self.optimizer.zero_grad()

            # Data from NewsUpdateDataset: input_ids, attention_mask, labels
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # Get embeddings using the base GTE model (no gradients needed here)
            with torch.no_grad():
                gte_outputs = self.embedding_model(input_ids=input_ids, attention_mask=attention_mask)
                embeddings = gte_outputs.last_hidden_state # [B, T, C]

            # --- Forward pass through the main adaptive model ---
            # Pass labels and training=True to update feature importance if applicable
            # Need autocast for mixed precision if using scaler
            # with autocast(enabled=scaler.is_enabled()): # Assuming scaler is defined globally or passed
            outputs = self.model(embeddings, attention_mask, labels=labels, training=True)

            # Calculate standard classification loss
            class_loss = self.criterion(outputs, labels)

            # Calculate EWC loss (if EWC is active)
            ewc_penalty = torch.tensor(0.0, device=self.device)
            if self.ewc and self.ewc_lambda &gt; 0:
                ewc_penalty = self.ewc.ewc_loss(self.ewc_lambda)

            # Combine losses
            loss = class_loss + ewc_penalty

            # --- Backward Pass &amp; Optimizer Step ---
            loss.backward()
            # Optional: Gradient clipping
            # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            # scaler.scale(loss).backward() # If using GradScaler
            # scaler.step(optimizer)
            # scaler.update()

            total_loss += loss.item()
            total_class_loss += class_loss.item()
            total_ewc_penalty += ewc_penalty.item()
            num_batches += 1
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'clf': f'{class_loss.item():.4f}',
                'ewc': f'{ewc_penalty.item():.4f}'})


        avg_loss = total_loss / num_batches if num_batches &gt; 0 else float('inf')
        avg_class_loss = total_class_loss / num_batches if num_batches &gt; 0 else float('inf')
        avg_ewc_penalty = total_ewc_penalty / num_batches if num_batches &gt; 0 else float('inf')
        print(f"Update finished. Avg Loss: {avg_loss:.4f}  (Avg Class Loss: {avg_class_loss:.4f}, Avg EWC Penalty: {avg_ewc_penalty:.4f})")

        # --- Post-Update Actions ---
        # Check if performance improved (lower average loss is better)
        is_best = avg_loss &lt; self.best_performance
        if is_best:
            print(f"Performance improved! ({self.best_performance:.4f} -&gt; {avg_loss:.4f}). Saving as best model.")
            self.best_performance = avg_loss
        else:
            print(f"Performance did not improve ({avg_loss:.4f} vs best {self.best_performance:.4f}).")


        # Save checkpoint (regular and best if improved)
        self.save_checkpoint(avg_loss, is_best=is_best)

        # --- Update EWC for the *next* cycle ---
        # Re-compute Fisher and save optimal params *after* the update is done,
        # using the data from *this* update cycle as the new reference.
        if self.ewc_lambda &gt; 0:
            print("Updating EWC Fisher matrix and optimal parameters for the *next* cycle...")
            # Create a new EWC instance with the updated model and the current update data
            self.ewc = EWC(self.model, update_dataloader, self.criterion, self.device, self.embedding_model)
            self.ewc.compute_fisher_information() # Compute Fisher based on the data just trained on
            self.ewc.save_optimal_params()      # Save the parameters *after* this update
            # Update the reference dataloader for the *next* potential EWC init
            self.ewc_reference_dataloader = update_dataloader


        # Analyze feature adaptation if model supports it
        adaptation_stats = None
        if isinstance(self.model, AdaptiveSparseModel):
            adaptation_stats = self.model.get_sparsity_info()
            if adaptation_stats:
                print("--- Feature Adaptation Stats After Update ---")
                print(f"  Stability (Jaccard): {adaptation_stats['feature_stability_jaccard']:.3f}")
                print(f"  Active Features: {adaptation_stats['active_features_count']}")
                print(f"  Importance (Mean/Std): {adaptation_stats['importance_stats_active']['mean']:.3f} / {adaptation_stats['importance_stats_active']['std']:.3f}")
                print(f"  History Points: {adaptation_stats['num_history_points']}")
                print("-" * 45)


        update_duration = time.time() - start_time
        print(f"Update cycle {self.current_update_cycle} completed in {update_duration:.2f} seconds.")

        return adaptation_stats # Return adaptation stats for logging/analysis

    def predict_with_confidence(self, text):
        """Makes a prediction for a single text instance, returns details."""
        if not isinstance(text, str):
            print("Error: Input must be a single string.")
            return None

        self.model.eval() # Ensure evaluation mode
        self.embedding_model.eval()
        active_features_info = {}

        with torch.no_grad():
            # 1. Tokenize
            try:
                # Use model's max length capability if available
                max_len = self.model.pos_encoder.pe.size(1)
            except AttributeError:
                max_len = 512 # Fallback max length
            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_len).to(self.device)

            # 2. Get Embeddings (GTE)
            gte_outputs = self.embedding_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            embeddings = gte_outputs.last_hidden_state # [1, T, C]

            # 3. Get Model Prediction (AdaptiveSparseModel or other)
            # Pass training=False explicitly
            logits = self.model(embeddings, inputs['attention_mask'], training=False) # [1, num_classes]
            probabilities = F.softmax(logits, dim=1) # [1, num_classes]
            confidence, prediction = torch.max(probabilities, dim=1) # Get max prob and its index

            # 4. Get Active Feature Info (if applicable)
            if isinstance(self.model, AdaptiveSparseModel):
                try:
                    active_features_indices = self.model.sparse_encoder.get_active_features()
                    active_features_info = {
                        'active_features_count': len(active_features_indices),
                        # Show top N active feature indices (convert to list)
                        'top_active_indices': active_features_indices[:20].cpu().tolist()
                    }
                except Exception as e:
                    print(f"Could not get active feature info: {e}")
                    active_features_info = {'active_features_count': 'Error'}


        return {
            'prediction': prediction.item(), # 0 or 1
            'confidence': confidence.item(), # Probability of predicted class
            'probabilities': probabilities.squeeze().cpu().tolist(), # All probabilities
        }
</code></pre>

    <h2>Run Continuous Pipeline</h2>
    <p>Defining the main function to orchestrate the continuous learning loop: fetching new data, making predictions, potentially labeling, updating the model, and analyzing adaptation.</p>
    <pre><code class="language-python">def run_continuous_pipeline(pipeline: ContinuousLearningPipeline, max_updates=5, min_new_data=5, wait_time_seconds=120):
    """Runs the continuous learning loop."""
    print("\n===== Starting Continuous Learning Pipeline =====")
    initial_cycle = pipeline.current_update_cycle # Track starting cycle
    target_cycle = initial_cycle + max_updates

    while pipeline.current_update_cycle &lt; target_cycle:
        current_cycle_display = pipeline.current_update_cycle + 1
        print(f"\n--- Update Cycle {current_cycle_display}/{target_cycle} ---")

        # 1. Fetch and process new potential data
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Fetching potential new data (keywords and fact-checks)...")
        keywords = get_recent_keywords() # Fetch recent keywords
        if not keywords:
            print("No keywords found from news API, skipping fact-check fetch for this cycle.")
            fact_checked_results = []
        else:
            fact_checked_results = get_fact_checked_news(keywords) # Fetch fact-checks for keywords

        if not fact_checked_results:
            print(f"No new relevant fact-checked articles found based on keywords.")
            # Decide whether to wait or potentially stop if no data is found for several cycles
            print(f"Waiting {wait_time_seconds} seconds before next attempt...")
            time.sleep(wait_time_seconds)
            continue # Skip to next iteration of the while loop

        # 2. Process new data: Predict and Prepare for Training
        new_texts_for_update = []
        new_labels_for_update = []
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Analyzing {len(fact_checked_results)} fetched claims...")

        for i, result in enumerate(fact_checked_results):
            text = result.get('claim_text')
            rating = result.get('rating') # Expecting lowercase rating e.g., 'false', 'true'
            url = result.get('review_url', 'N/A')
            lang = result.get('language', 'N/A')

            if not text or not rating:
                # print(f"Skipping incomplete fact-check (URL: {url})")
                continue

            # Make prediction with current model
            prediction_result = pipeline.predict_with_confidence(text)
            if prediction_result is None: continue # Skip if prediction failed

            # Optional: Print details for analysis
            if i &lt; 5 or i % 10 == 0: # Print details for first few and every 10th
                print(f"\nAnalyzing Claim {i+1}/{len(fact_checked_results)} ({lang}):")
                print(f"  Claim: {text[:150].strip()}...") # Print truncated claim
                # print(f"  URL: {url}")
                predicted_label_str = 'Fake' if prediction_result['prediction'] == 1 else 'Real' # Assuming 1=Fake, 0=Real
                print(f"  Model Prediction: {predicted_label_str} (Conf: {prediction_result['confidence']:.3f})")
                if 'active_features_count' in prediction_result:
                    active_count = prediction_result['active_features_count']
                    print(f"  Active Features: {active_count if isinstance(active_count, int) else 'N/A'}")


            # --- Determine ground truth label from rating ---
            # This logic needs to be robust based on the specific API ratings
            label_int = None
            actual_label_str = None
            # Common English variations for Fake
            if any(term in rating for term in ['false', 'fake', 'incorrect', 'unproven', 'baseless', 'misleading', 'pants on fire']):
                label_int = 1 # Fake
                actual_label_str = "Fake"
            # Common Arabic variations for Fake
            elif any(term in rating for term in ['غير صحيح', 'زائف', 'خاطئ', 'مضلل', 'لا أساس له']):
                label_int = 1 # Fake
                actual_label_str = "Fake"
            # Common English variations for Real
            elif any(term in rating for term in ['true', 'correct', 'accurate', 'mostly true', 'verified']):
                label_int = 0 # Real
                actual_label_str = "Real"
            # Common Arabic variations for Real
            elif any(term in rating for term in ['صحيح', 'دقيق', 'حقيقي', 'متحقق منه', 'غالبًا صحيح']):
                label_int = 0 # Real
                actual_label_str = "Real"
            # Handle other ratings ('mixture', 'outdated', 'satire' etc.) - skip update for these?
            else:
                if i &lt; 5 or i % 10 == 0: print(f"  Actual Label: Unknown/Other ('{rating}') - Skipping for update")

            # If a valid label (0 or 1) was determined, add to update lists
            if label_int is not None:
                if i &lt; 5 or i % 10 == 0: print(f"  Actual Label: {actual_label_str} (Rating: '{rating}') - Adding for update.")
                new_texts_for_update.append(text)
                new_labels_for_update.append(label_int)

                # Optional: Log discrepancies for analysis
                if prediction_result['prediction'] != label_int:
                    if i &lt; 5 or i % 10 == 0: print("    &gt;&gt; Prediction Mismatch!")


        # 3. Update model if enough new labeled data is available
        num_new_items = len(new_texts_for_update)
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Found {num_new_items} valid items for update.")

        if num_new_items &gt;= min_new_data:
            print(f"Triggering model update with {num_new_items} items (minimum was {min_new_data}).")
            try:
                adaptation_stats = pipeline.update_model(new_texts_for_update, new_labels_for_update)
                # Log adaptation_stats if needed or returned
            except Exception as e:
                print(f"!!!!!!!!!!!!! Error during pipeline.update_model: {e} !!!!!!!!!!!!!")
                # import traceback
                # print(traceback.format_exc())
                print("Skipping update for this cycle due to error.")
        else:
            print(f"Not enough new labeled data ({num_new_items} &lt; {min_new_data}) to trigger model update this cycle.")
            # Increment cycle count even if no update happened, to avoid infinite loop if data is scarce
            # pipeline.current_update_cycle += 1 # Or handle this differently

        # Optional: Add delay between cycles unless target is reached
        if pipeline.current_update_cycle &lt; target_cycle:
            print(f"Waiting {wait_time_seconds} seconds before next cycle...")
            time.sleep(wait_time_seconds) # Wait before next iteration

    print(f"\n===== Continuous Learning Pipeline Finished ({pipeline.current_update_cycle} cycles completed) =====")


# --- Example Execution Block ---
if __name__ == "__main__":
    # Ensure required variables like device, MODEL_SAVE_PATH, etc., are defined
    # from the previous sections of the script.

    # Check if the initial model path exists
    if not os.path.exists(MODEL_SAVE_PATH):
        print(f"Error: Initial model checkpoint '{MODEL_SAVE_PATH}' not found. Cannot start pipeline.")
    else:
        print("\n--- Initializing Continuous Learning Pipeline ---")
        # Instantiate the pipeline
        # Use the base_model_path of the *best* model from initial training
        continuous_pipeline = ContinuousLearningPipeline(
            model_class=AdaptiveSparseModel, # Specify the model class
            base_model_path=MODEL_SAVE_PATH, # Path to best initially trained model
            save_dir='model_checkpoints_continuous', # Directory for continuous checkpoints
            ewc_lambda=5000.0, # Tune this regularization strength
            learning_rate=1e-5, # Learning rate for updates
            update_batch_size=16,
            device=device,
            tokenizer=byt5_tokenizer, # Pass tokenizer
            embedding_model=gte_model # Pass GTE model
            # Add other relevant args for AdaptiveSparseModel if needed
        )

        # --- Run the pipeline ---
        run_continuous_pipeline(
            pipeline=continuous_pipeline,
            max_updates=3,      # Number of update cycles to run
            min_new_data=5,     # Minimum new items needed to trigger an update
            wait_time_seconds=60 # Wait time between cycles if no data/update
        )
</code></pre>

    <h2>Prediction and Justification (LIME &amp; SHAP)</h2>
    <p>Defining a function to make a prediction for a given text and generate explanations using both LIME and SHAP frameworks.</p>
    <pre><code class="language-python"># --- Ensure necessary models are loaded and on device ---

# Determine which model to use for justification:
# Option 1: Use the latest best model from continuous learning
continuous_save_dir = 'model_checkpoints_continuous'
best_cont_path = os.path.join(continuous_save_dir, 'best_model_checkpoint.pth')

# Option 2: Fallback to the initial best model if continuous hasn't run or saved a best model
initial_best_path = MODEL_SAVE_PATH # Defined earlier

final_model_path_to_load = None
model_to_justify = None

if os.path.exists(best_cont_path):
    final_model_path_to_load = best_cont_path
    print(f"Using best model from continuous learning: {final_model_path_to_load}")
    # Instantiate the correct model class (likely AdaptiveSparseModel)
    try:
        # Need to know the class used in the pipeline
        # Assuming AdaptiveSparseModel was used
        model_to_justify = AdaptiveSparseModel(
            embedding_dim=gte_model.config.hidden_size,
            # Add other necessary args used during pipeline init
            device=device
        ).to(device)
    except NameError:
        print("Error: AdaptiveSparseModel class not defined. Cannot instantiate.")
        model_to_justify = None
    except Exception as e:
        print(f"Error instantiating AdaptiveSparseModel: {e}")
        model_to_justify = None

elif os.path.exists(initial_best_path):
    final_model_path_to_load = initial_best_path
    print(f"Continuous learning checkpoint not found. Using model from initial training: {final_model_path_to_load}")
    # Instantiate the initial model class (likely CombinedModel)
    try:
        model_to_justify = CombinedModel(
            embedding_dim=gte_model.config.hidden_size,
            # Add other necessary args used during initial training
        ).to(device)
    except NameError:
        print("Error: CombinedModel class not defined. Cannot instantiate.")
        model_to_justify = None
    except Exception as e:
        print(f"Error instantiating CombinedModel: {e}")
        model_to_justify = None
else:
    print("Error: No suitable model checkpoint found for justification.")
    # Exit or handle this case appropriately
    # sys.exit("Cannot proceed without a model.") # Example exit

# Load the state dict if model and path are valid
if model_to_justify is not None and final_model_path_to_load:
    try:
        checkpoint = torch.load(final_model_path_to_load, map_location=device)
        # Use strict=False for flexibility, especially if loading into a different class structure sometimes
        missing_keys, unexpected_keys = model_to_justify.load_state_dict(
            checkpoint['model_state_dict'], strict=False
        )
        print("Final model state loaded for justification.")
        if missing_keys: print(f"  - Missing keys: {missing_keys}")
        if unexpected_keys: print(f"  - Unexpected keys: {unexpected_keys}")
        model_to_justify.eval() # Set to evaluation mode
        gte_model.eval()      # Ensure GTE is also in eval mode
    except Exception as e:
        print(f"Error loading final model state dict from {final_model_path_to_load}: {e}")
        model_to_justify = None # Invalidate model if loading failed
else:
    model_to_justify = None # Ensure it's None if instantiation failed


# --- Prediction Function Wrappers for LIME/SHAP ---
# This function takes raw text(s) and returns model probabilities [N, num_classes]
# It needs access to the final loaded `model_to_justify`

def model_predict_proba_wrapper(texts):
    """Callable for LIME/SHAP that handles tokenization, embedding, and prediction."""
    if model_to_justify is None:
        raise RuntimeError("Model for justification has not been loaded successfully.")

    model_to_justify.eval()
    gte_model.eval()

    # Ensure input is a list of strings
    if isinstance(texts, str): texts = [texts]
    # Convert numpy array or other iterables to list of strings
    if not isinstance(texts, list): texts = [str(t) for t in texts]
    else: texts = [str(t) for t in texts] # Ensure all elements are strings

    all_probs = []
    # Process in batches if input list is large? (LIME/SHAP usually send small lists/single items)
    # Simple non-batched version:
    with torch.no_grad():
        for text in texts:
            if not text.strip(): # Handle empty strings
                # Return uniform probabilities or handle as error?
                # Assuming 2 classes [0.5, 0.5] for empty input
                num_classes = model_to_justify.classifier.fc2.out_features
                all_probs.append(np.full((1, num_classes), 1.0 / num_classes))
                continue

            # Tokenize
            try:
                max_len = model_to_justify.pos_encoder.pe.size(1)
            except AttributeError:
                max_len = 512 # Fallback
            inputs = byt5_tokenizer(text, return_tensors='pt', padding='max_length', # Pad for consistency
                                    truncation=True, max_length=max_len).to(device)
            # Embed
            gte_outputs = gte_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            embeddings = gte_outputs.last_hidden_state
            # Predict
            # Pass training=False explicitly
            logits = model_to_justify(embeddings, inputs['attention_mask'], training=False)
            probabilities = F.softmax(logits, dim=1)
            all_probs.append(probabilities.cpu().numpy()) # Move to CPU for numpy array

    if not all_probs: # Handle empty input list case
        num_classes = model_to_justify.classifier.fc2.out_features
        return np.array([]).reshape(0, num_classes)

    # Stack results into [N, num_classes] numpy array
    return np.vstack(all_probs)


# --- Main Justification Function ---
def predict_and_justify_with_lime_and_shap(text_instance, num_lime_features=10, num_lime_samples=1000):
    """ Predicts and generates LIME/SHAP explanations for a text instance. """
    if model_to_justify is None:
        print("Error: Model for justification is not available.")
        return None

    print(f"\n--- Generating Justification for: '{text_instance[:100].strip()}...' ---")

    # 1. Get Model Prediction (using the wrapper)
    try:
        probabilities = model_predict_proba_wrapper(text_instance)
        if probabilities.shape[0] == 0:
            print("Error: Prediction function returned empty result.")
            return None
        # Prediction for the first (and only) text instance
        prediction = np.argmax(probabilities[0]) # Get class index
        pred_prob = probabilities[0, prediction]
        # Assuming 0=Real, 1=Fake
        predicted_class_name = ['Real', 'Fake'][prediction]

        print(f"Prediction: {predicted_class_name} (Probability: {pred_prob:.4f})")
        print(f"Probabilities [Real, Fake]: [{probabilities[0, 0]:.4f}, {probabilities[0, 1]:.4f}]")

    except Exception as e:
        print(f"Error during prediction for justification: {e}")
        return None


    lime_explanation_obj = None
    shap_values_obj = None
    lime_html_path = None

    # 2. LIME Explanation
    print("\nGenerating LIME explanation...")
    try:
        class_names_lime = ['Real', 'Fake'] # Must match model output order
        explainer_lime = LimeTextExplainer(class_names=class_names_lime, bow=False) # Use word vectors or fragments
        lime_explanation_obj = explainer_lime.explain_instance(
            text_instance,
            model_predict_proba_wrapper, # Use the wrapper function
            num_features=num_lime_features,
            num_samples=num_lime_samples, # Number of samples for LIME perturbation
            labels=[0, 1] # Explain probabilities for both classes
        )
        print("LIME explanation generated.")
        # Save LIME explanation to HTML file
        lime_html_path = f'lime_explanation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html'
        lime_explanation_obj.save_to_file(lime_html_path)
        print(f"LIME explanation saved to {lime_html_path}")

    except Exception as e:
        print(f"Error during LIME explanation: {e}")
        # import traceback
        # print(traceback.format_exc())


    # 3. SHAP Explanation (using KernelExplainer as a fallback, can be SLOW)
    # print("\nGenerating SHAP explanation (using KernelExplainer - may be slow)...")
    # try:
    #     # KernelExplainer needs background data for approximation.
    #     # Using a small sample of the training/validation data is common.
    #     # Creating dummy background data here - REPLACE with actual data sample.
    #     # Need to ensure background data is in the format expected by the masker (raw text).
    #     background_data_sample = ["some neutral text", "another example sentence"] # VERY IMPORTANT: Use representative data
    #     if 'combined_train_df' in globals(): # Try to use actual data if available
    #          background_data_sample = shap.sample(combined_train_df[text_column].tolist(), 50).tolist() # Sample 50 texts
    #          print(f"Using {len(background_data_sample)} samples from training data as SHAP background.")

    #     # explainer_shap = shap.KernelExplainer(model_predict_proba_wrapper, background_data_sample)

    #     # Alternative: PartitionExplainer (often better for text/NLP)
    #     # Requires model function and masker
    #     masker_shap = shap.maskers.Text(r"\W+") # Simple whitespace tokenizer masker

    #     explainer_shap = shap.PartitionExplainer(model_predict_proba_wrapper, masker_shap,
    #                                             output_names=['Real', 'Fake'])

    #     # Generate SHAP values for the specific text instance
    #     shap_values_obj = explainer_shap([text_instance]) # Input should be a list
    #     print("SHAP explanation generated.")

    #     # Optional: Create and display SHAP plots (might require matplotlib)
    #     # try:
    #     #     # Plot for the predicted class
    #     #     shap.plots.text(shap_values_obj[0, :, prediction], display=True) # Requires specific shap_values format
    #     #     plt.title(f"SHAP explanation for {predicted_class_name}")
    #     #     plt.show()
    #     # except Exception as plot_e:
    #     #     print(f"Could not generate SHAP plot: {plot_e}")


    # except Exception as e:
    #     print(f"Error during SHAP explanation: {e}")
    #     # import traceback
    #     # print(traceback.format_exc())


    # 4. Return Results
    results = {
        'text': text_instance,
        'prediction': prediction,
        'predicted_class_name': predicted_class_name,
        'probability_predicted': pred_prob,
        'probabilities_all': probabilities[0].tolist(), # List of [prob_real, prob_fake]
        'lime_explanation': lime_explanation_obj, # The LIME explanation object
        'lime_html_path': lime_html_path,       # Path to the saved LIME HTML file
        'shap_values': shap_values_obj         # The SHAP values object (might be None)
    }

    return results


# --- Example Usage for Justification ---
if __name__ == "__main__":
    # Ensure the script has run up to this point, loading models etc.
    if model_to_justify: # Check if the model was loaded successfully
        # Example Arabic claim (replace with any text)
        sample_claim = "دراسة علمية حديثة تثبت أن شرب الماء البارد يسبب السرطان بشكل مباشر"
        # Example English claim
        # sample_claim = "Study finds that cold water directly causes cancer."

        justification_results = predict_and_justify_with_lime_and_shap(sample_claim, num_lime_features=12, num_lime_samples=1500)

        if justification_results:
            # Access results (e.g., print LIME features)
            if justification_results['lime_explanation']:
                print("\n--- Top LIME Features ---")
                pred_label_idx = justification_results['prediction']
                pred_label_name = justification_results['predicted_class_name']
                try:
                    # Access features for the predicted class
                    lime_features = justification_results['lime_explanation'].as_list(label=pred_label_idx)
                    print(f"Features contributing to '{pred_label_name}' prediction:")
                    for feature, weight in lime_features:
                        print(f"  - '{feature}': {weight:.4f}")

                      # Optionally show features for the *other* class
                    other_label_idx = 1 - pred_label_idx
                    other_label_name = ['Real', 'Fake'][other_label_idx]
                    lime_features_other = justification_results['lime_explanation'].as_list(label=other_label_idx)
                    print(f"\nFeatures contributing to '{other_label_name}' prediction:")
                    for feature, weight in lime_features_other:
                        print(f"  - '{feature}': {weight:.4f}")


                except Exception as e:
                    print(f"Could not extract LIME features list: {e}")
            else:
                print("\nLIME explanation generation failed or was skipped.")

            if justification_results['shap_values'] is not None:
                print("\nSHAP explanation generated (values object available).")
                # Add code here to process or visualize SHAP values if needed.
                # The structure of `shap_values` depends on the explainer used.
                # Example for PartitionExplainer/KernelExplainer output:
                # try:
                #      shap_vals_for_pred_class = justification_results['shap_values'][0, :, pred_label_idx]
                #      # shap_vals_for_pred_class.values -&gt; array of SHAP values
                #      # shap_vals_for_pred_class.data -&gt; array of original tokens/features
                #      print(f"SHAP base value for {pred_label_name}: {shap_vals_for_pred_class.base_values:.4f}")
                #      # Print top contributing words/tokens from SHAP
                #      word_indices = np.argsort(np.abs(shap_vals_for_pred_class.values))[::-1] # Sort by abs value desc
                #      print(f"Top SHAP contributors to '{pred_label_name}':")
                #      for i in range(min(10, len(word_indices))): # Print top 10
                #           idx = word_indices[i]
                #           word = shap_vals_for_pred_class.data[idx]
                #           shap_val = shap_vals_for_pred_class.values[idx]
                #           print(f"  - '{word}': {shap_val:.4f}")
                # except Exception as e:
                #      print(f"Could not process SHAP values object: {e}")

            else:
                print("\nSHAP explanation generation failed or was skipped.")
        else:
            print("Justification process failed.")

    else:
        print("Skipping justification example because the model could not be loaded.")
</code></pre>

    <h2>Final Example Usage</h2>
    <p>Demonstrating how to use the trained model (potentially after continuous learning) and the justification function with a new claim.</p>
    <pre><code class="language-python">if __name__ == "__main__":
    # Check if the model is loaded before running this example
    if 'model_to_justify' in globals() and model_to_justify is not None:
        # Example new claim to test (Arabic)
        new_claim_example = """
        اغتيال عالم الكيمياء الدكتور حمدي عبد العظيم في منزله والشكوك تحوم حول دوافع سياسية
        """ # Translation: Assassination of chemist Dr. Hamdi Abdel Azim in his home, suspicions revolve around political motives

        # Example new claim (English)
        # new_claim_example = "BREAKING: Famous scientist found dead, political foul play suspected by investigators."

        print("\n\n===== Running Final Justification Example =====")
        # Run the justification function
        final_results = predict_and_justify_with_lime_and_shap(new_claim_example)

        if final_results:
            print("\n----- Final Results Summary -----")
            print(f"Claim: {final_results['text'].strip()}")
            print(f"Predicted Class: {final_results['predicted_class_name']}")
            print(f"Prediction Probability: {final_results['probability_predicted']:.4f}")
            print(f"Probabilities [Real, Fake]: {final_results['probabilities_all']}") # Print both probabilities

            if final_results['lime_explanation']:
                print(f"\nLIME explanation generated and saved to: {final_results['lime_html_path']}")
                # Optionally print top features again if desired
                pred_idx = final_results['prediction']
                try:
                    print("Top LIME Features for prediction:")
                    features = final_results['lime_explanation'].as_list(label=pred_idx)
                    for f, w in features[:5]: # Print top 5
                        print(f"  - '{f}': {w:.4f}")
                except Exception as e:
                    print(f"Could not display LIME features: {e}")
            else:
                print("\nLIME explanation was not generated successfully.")

            if final_results['shap_values']:
                print("\nSHAP explanation generated (values object available).")
                # Add any summary/processing of SHAP values desired here
            else:
                print("\nSHAP explanation was not generated successfully or skipped.")

            print("\n============================================")
        else:
            print("\nFailed to get justification results for the final example.")

    else:
        print("\nSkipping final justification example as the model is not loaded.")
</code></pre>

</body>
</html>
